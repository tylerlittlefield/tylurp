<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Programming on tyluRp</title>
    <link>/categories/programming/</link>
    <description>Recent content in Programming on tyluRp</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 28 Apr 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/programming/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>File to folder</title>
      <link>/2018/04/28/file-to-folder/</link>
      <pubDate>Sat, 28 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/28/file-to-folder/</guid>
      <description>I recently needed to create folders for every file in a directory, name these folders after the file, and move the file to the correct folder. The directory I was working with had over 1,000 files so I wasn&amp;rsquo;t going to do this by hand. Instead I discovered a way to automate this process using a shell script.
For example, navigate to a directory and then run the following:
for i in *; do extension=&amp;quot;${i##*.</description>
    </item>
    
    <item>
      <title>HaRvest data from the web</title>
      <link>/2018/04/28/harvest-data-from-the-web/</link>
      <pubDate>Sat, 28 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/28/harvest-data-from-the-web/</guid>
      <description>Intro There&amp;rsquo;s a cool package called rvest which makes scraping data from the web pretty easy. This package is designed to work with magrittr so users get the benefits that come along with the %&amp;gt;% operator.
I&amp;rsquo;ll start with an example using a wikipedia page that points to Sleepy Hollow, New York.
Sleepy Hollow, NY Below is the full code for scraping and cleaning the data:
library(tidyverse) library(rvest) # Scrape population data from wikipedia &amp;quot;https://en.</description>
    </item>
    
  </channel>
</rss>