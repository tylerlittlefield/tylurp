<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on tyluRp</title>
    <link>/post/</link>
    <description>Recent content in Posts on tyluRp</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 02 Sep 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>treeco: eco benefits in R</title>
      <link>/2018/09/02/treeco-eco-benefits-in-r/</link>
      <pubDate>Sun, 02 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/09/02/treeco-eco-benefits-in-r/</guid>
      <description>Update: I&amp;rsquo;ve added a pkgdown site for this package, lots more info can be found at https://treeco.netlify.com/
I&amp;rsquo;m excited to say that I&amp;rsquo;ve developed my first R package treeco, a tool for calculating the eco benefits of trees. To my suprise, it&amp;rsquo;s not that difficult to actually develope a package. With tools like usethis which makes the developement process easy, the main challenge is simply coming up with an idea.</description>
    </item>
    
    <item>
      <title>File to folder</title>
      <link>/2018/04/28/file-to-folder/</link>
      <pubDate>Sat, 28 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/28/file-to-folder/</guid>
      <description>I recently needed to create folders for every file in a directory, name these folders after the file, and move the file to the correct folder. The directory I was working with had over 1,000 files so I wasn&amp;rsquo;t going to do this by hand. Instead I discovered a way to automate this process using a shell script.
For example, navigate to a directory and then run the following:
for i in *; do extension=&amp;quot;${i##*.</description>
    </item>
    
    <item>
      <title>HaRvest data from the web</title>
      <link>/2018/04/28/harvest-data-from-the-web/</link>
      <pubDate>Sat, 28 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/28/harvest-data-from-the-web/</guid>
      <description>Intro There&amp;rsquo;s a cool package called rvest which makes scraping data from the web pretty easy. This package is designed to work with magrittr so users get the benefits that come along with the %&amp;gt;% operator.
I&amp;rsquo;ll start with an example using a wikipedia page that points to Sleepy Hollow, New York.
Sleepy Hollow, NY Below is the full code for scraping and cleaning the data:
library(tidyverse) library(rvest) # Scrape population data from wikipedia &amp;quot;https://en.</description>
    </item>
    
    <item>
      <title>Equality Matrix, a useful idea for benchmarking</title>
      <link>/2018/04/03/equality-matrix-a-useful-idea-for-benchmarking/</link>
      <pubDate>Tue, 03 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/04/03/equality-matrix-a-useful-idea-for-benchmarking/</guid>
      <description>This post is essentially an extension of a previous post I wrote. The only addition is a cool idea for testing near equality of outputs. I&amp;rsquo;m calling this an &amp;ldquo;equality matrix&amp;rdquo;, a matrix of methods that displays whether or not they are equal to each other. One use case is benchmarking. As we benchmark multiple solutions to a single problem, testing whether or not the outputs are equal becomes more time consuming as the all.</description>
    </item>
    
    <item>
      <title>Measuring performance</title>
      <link>/2018/02/13/measuring-performance/</link>
      <pubDate>Tue, 13 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/13/measuring-performance/</guid>
      <description>With any data science project there may be times where performance is a concern. This might be a result of the datas size or a lack of resources. In any case, we can use microbenchmark to measure the performance of specific functions.1
I discovered this package on Stack Overflow as someone measured the performance of their answer among others. Hadley Wickham also writes about microbenchmark in his Advanced R book.</description>
    </item>
    
    <item>
      <title>drake: build workflows with R</title>
      <link>/2018/02/03/drake-build-workflows-with-r/</link>
      <pubDate>Sat, 03 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/03/drake-build-workflows-with-r/</guid>
      <description>Introduction drake is a really cool R package designed to manage workflows. It’s described as a computational engine for data science projects. The goal is to keep results up to date while also skipping anything that hasn’t changed. As a result, users can scale their work, skip redundant work, and reap the benefits of reproducibility.
 “What gets done stays done.”
 To no suprise, the package is part of the rOpenSci project, a community devoted to transparent research practices and reproducibility.</description>
    </item>
    
    <item>
      <title>Reproducibility</title>
      <link>/2018/01/30/reproducibility/</link>
      <pubDate>Tue, 30 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/01/30/reproducibility/</guid>
      <description>“Welcome to SO. In order to answer your question, please provide a minimal, complete, and verifiable example.”
 If you’ve ever asked a question on Stack Overflow (SO), you may recognize the quote above. This is because reproducibility is incredibly important when asking programming questions and it’s often overlooked by newcombers. To be brief, a great reproducible example should be:
Minimal – Provide the least amount of code needed to reproduce the problem.</description>
    </item>
    
  </channel>
</rss>